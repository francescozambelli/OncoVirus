{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import graph_tool as gt\n",
    "from tqdm import tqdm\n",
    "from operator import itemgetter\n",
    "from functools import reduce\n",
    "import itertools\n",
    "import scipy.sparse as sps\n",
    "import random\n",
    "import os\n",
    "\n",
    "import MuxVizPy as mxp\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "working_dir = \"../DataMNT/\"\n",
    "\n",
    "# multilayer settings\n",
    "layerCouplingStrength = 1\n",
    "networkOfLayersType = \"categorical\" ## = all to all\n",
    "\n",
    "#virus metadata\n",
    "virus_metadata = pd.read_csv(working_dir+\"/Files/viruses_metadata.csv\", header=0, sep=\";\")\n",
    "\n",
    "virus_metadata_onco = virus_metadata[virus_metadata[\"isOncogenic\"] == True].reset_index()\n",
    "virus_metadata_nonco = virus_metadata[virus_metadata[\"isOncogenic\"] == False].reset_index()\n",
    "\n",
    "#dictionary containing a unquie mapping between name of the protein and a corresponding index\n",
    "node_map_df = pd.read_csv(working_dir+\"/Files/node_map.csv\")\n",
    "node_map_dict = {k:(v-1) for k,v in zip(node_map_df[\"Prot\"], node_map_df[\"Index\"])}\n",
    "\n",
    "#function to create list of n_iter combination of nonco virus indexes with a fixed random seed for repitibility\n",
    "def SamplingForNoco(n_iter, start=0, group_dim=8, random_seed=1234):\n",
    "    np.random.seed(random_seed)\n",
    "    nonco_cond = np.where(np.all([np.array(virus_metadata[\"virus\"]!=\"Human_SARS_coronavirus_2\"),\n",
    "                                  np.array(virus_metadata[\"virus_short\"]!=\"Lymphocytic_choriomeningitis_virus\"),\n",
    "                                  np.array(virus_metadata[\"neigh_order\"]==NEIGH_ORDER), \n",
    "                                  np.array(virus_metadata[\"isOncogenic\"]==False)],\n",
    "                                  axis=0))\n",
    "    \n",
    "    nonco_sampling = np.array([np.random.choice(nonco_cond[0], group_dim, replace=False) for i in range(n_iter+start)])\n",
    "    return nonco_sampling[start:(n_iter+start)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_iters = 256\n",
    "np.random.seed(100)\n",
    "\n",
    "#position of the sars cov-2 in the metadata file\n",
    "Sars_pos = np.where(np.array(np.all([virus_metadata[\"neigh_order\"]==NEIGH_ORDER, virus_metadata[\"virus\"]==\"Human_SARS_coronavirus_2\"], axis=0)))[0][0]\n",
    "#positions of the oncogenic viruses in the metadata file\n",
    "onco_virus_indexes = np.where(np.array(np.all([virus_metadata[\"neigh_order\"]==NEIGH_ORDER, virus_metadata[\"isOncogenic\"] == True], axis=0)))[0]\n",
    "\n",
    "### creating combinations of indexes referred to specific viruses to build each sample for each combination set ###\n",
    "\n",
    "#N\n",
    "n_virus_indexes = SamplingForNoco(n_iters, group_dim=4, random_seed=41252145)\n",
    "\n",
    "\n",
    "#N1O\n",
    "n1o_virus_indexes = []\n",
    "n1o_sampling = SamplingForNoco(n_iters, group_dim=3, random_seed=456)\n",
    "for i in range(len(n1o_sampling)):\n",
    "    onco_pick = np.random.choice(onco_virus_indexes)\n",
    "    n1o_virus_indexes.append(np.concatenate([n1o_sampling[i], [onco_pick]]))\n",
    "\n",
    "#N1S\n",
    "Snonco_nonco_samples = SamplingForNoco(n_iters, group_dim=3, random_seed=4563)\n",
    "n1s_virus_indexes = np.concatenate([Snonco_nonco_samples, np.repeat(Sars_pos,n_iters).reshape([n_iters,1])], axis=1)\n",
    "    \n",
    "#N2O\n",
    "n2o_virus_indexes = []\n",
    "n2o_sampling = SamplingForNoco(n_iters, group_dim=2, random_seed=17521)\n",
    "for i in range(len(n2o_sampling)):\n",
    "    onco_pick = np.random.choice(onco_virus_indexes, 2)\n",
    "    n2o_virus_indexes.append(np.concatenate([n2o_sampling[i], onco_pick]))\n",
    "    \n",
    "#N3O\n",
    "n3o_virus_indexes = []\n",
    "n3o_sampling = SamplingForNoco(n_iters, group_dim=3, random_seed=17521)\n",
    "for i in range(len(n2o_sampling)):\n",
    "    onco_pick = np.random.choice(onco_virus_indexes)\n",
    "    n3o_virus_indexes.append(np.concatenate([n3o_sampling[i], [onco_pick]]))\n",
    "\n",
    "#O\n",
    "comb = list(itertools.combinations(range(8), 4))\n",
    "o_virus_indexes = np.array([list(onco_virus_indexes[list(comb[i])]) for i in range(len(comb))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n': 256, 'n1o': 256, 'n2o': 256, 'n3o': 256, 'o': 70}\n"
     ]
    }
   ],
   "source": [
    "#organize in lists to better handle later\n",
    "\n",
    "index_lists = [n_virus_indexes,\n",
    "               n1o_virus_indexes,\n",
    "               n2o_virus_indexes, \n",
    "               n3o_virus_indexes,\n",
    "               o_virus_indexes\n",
    "               ]\n",
    "\n",
    "names_lists=[\"n\", \n",
    "             \"n1o\", \n",
    "             \"n2o\", \n",
    "             \"n3o\",\n",
    "             \"o\"\n",
    "             ]\n",
    "\n",
    "print(dict(zip(names_lists, [len(ioo) for ioo in index_lists])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the indexes combination for reproducibility\n",
    "for i in range(len(index_lists)):\n",
    "    np.savetxt(X=index_lists[i], fname=working_dir+\"/topology/indexes/\"+names_lists[i]+\".txt\", fmt=\"%d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topological quantities extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#computing LVC size for each sample of each combination set and print on file\n",
    "\n",
    "#for original (not randomized) human PPI network case\n",
    "if not os.path.isdir(working_dir+\"/topology/LVC/original\"):\n",
    "    os.mkdir(working_dir+\"/topology/LVC/original\")\n",
    "for nam, lst in zip(names_lists, index_lists):\n",
    "    print(nam)\n",
    "    lvc_size=[]\n",
    "    for i in tqdm(range(len(lst))):\n",
    "        net = mxp.VirusMultiplex_from_dirlist([working_dir+\"/SynteticViruses/original/\"+a for a in virus_metadata.iloc[lst[i]][\"virus\"]])\n",
    "        \n",
    "        lvc_curr = mxp.topology.get_multi_LVC(net.g_list, printt=False)\n",
    "\n",
    "        if type(lvc_curr)==int:\n",
    "            lvc_curr = [lvc_curr]\n",
    "        lvc_size.append(len(lvc_curr))\n",
    "\n",
    "    np.savetxt(X=lvc_size, fname=working_dir+\"/topology/LVC/original/\"+nam+\"_lvc.txt\", fmt=\"%d\")\n",
    "\n",
    "#for each realization of the randomization of the human PPI network\n",
    "for k in range(1,500):\n",
    "    if not os.path.isdir(working_dir+\"/topology/LVC/\"+str(k)):\n",
    "        os.mkdir(working_dir+\"/topology/LVC/\"+str(k))\n",
    "    for nam, lst in zip(names_lists, index_lists):\n",
    "        print(nam)\n",
    "        lvc_size=[]\n",
    "        for i in tqdm(range(len(lst))):\n",
    "            net = mxp.VirusMultiplex_from_dirlist([working_dir+\"/SynteticViruses/\"+str(k)+\"/\"+a for a in virus_metadata.iloc[lst[i]][\"virus\"]])\n",
    "            \n",
    "            lvc_curr = mxp.topology.get_multi_LVC(net.g_list, printt=False)\n",
    "    \n",
    "            if type(lvc_curr)==int:\n",
    "                lvc_curr = [lvc_curr]\n",
    "            lvc_size.append(len(lvc_curr))\n",
    "    \n",
    "        np.savetxt(X=lvc_size, fname=working_dir+\"/topology/LVC/\"+str(k)+\"/\"+nam+\"_lvc.txt\", fmt=\"%d\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the list of nodes in the LVCs\n",
    "# the resulting file will be used for the GO enrichment analysis\n",
    "\n",
    "# original network\n",
    "net = mxp.VirusMultiplex_from_dirlist([working_dir+\"/SynteticViruses/original/\"+a for a in virus_metadata.iloc[onco_virus_indexes][\"virus\"]])  \n",
    "lvc_curr = mxp.topology.get_multi_LVC(net.g_list, printt=False)\n",
    "np.savetxt(X=np.array(list(net.node_map))[lvc_curr], fname=working_dir+\"/GOdata/genes.list\", fmt=\"%s\")\n",
    "\n",
    "# saving union of nodes in onco layers of the original net\n",
    "# to be used as background gene set for the GO enrichment analysis\n",
    "#np.savetxt(X=list(net.node_map.keys()), fname=working_dir+\"/GOdata/gobackground.list\", fmt=\"%s\")\n",
    "\n",
    "#randomized networks\n",
    "for k in tqdm(range(500)):\n",
    "    net = mxp.VirusMultiplex_from_dirlist([working_dir+\"/SynteticViruses/\"+str(k)+\"/\"+a for a in virus_metadata.iloc[onco_virus_indexes][\"virus\"]])\n",
    "            \n",
    "    lvc_curr = mxp.topology.get_multi_LVC(net.g_list, printt=False)\n",
    "    lvc_synt=np.array(list(net.node_map))[lvc_curr]\n",
    "    np.savetxt(X=lvc_synt, fname=working_dir+\"/GOdata/Synt/genes_\"+str(k)+\".list\", fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Percolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 256/256 [02:33<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n1o\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 256/256 [04:14<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n2o\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 256/256 [05:36<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n3o\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 256/256 [04:00<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 70/70 [02:33<00:00,  2.19s/it]\n"
     ]
    }
   ],
   "source": [
    "# computing multi pagerank node percolation critical point for samples from different combination sets\n",
    "\n",
    "#in the case of original network\n",
    "if not os.path.isdir(working_dir+\"/topology/perc/original\"):\n",
    "    os.mkdir(working_dir+\"/topology/perc/original\")\n",
    "for nam, lst in zip(names_lists, index_lists):\n",
    "    print(nam)\n",
    "    perc_list=[]\n",
    "    for i in tqdm(range(len(lst))):\n",
    "        net = mxp.VirusMultiplex_from_dirlist([working_dir+\"/SynteticViruses/original/\"+a for a in virus_metadata.iloc[lst[i]][\"virus\"]])\n",
    "        \n",
    "        tensor=mxp.build.get_node_tensor_from_network_list(net.g_list)\n",
    "\n",
    "        order = mxp.versatility.get_multi_RW_centrality_edge_colored(tensor)\n",
    "        order = order.sort_values(\"vers\")[\"phy nodes\"].to_numpy()\n",
    "        g_agg = mxp.build.get_aggregate_network(tensor)\n",
    "\n",
    "        perc_agg_2 = gt.topology.vertex_percolation(g_agg, order, second=True)[0]\n",
    "        max_perc = np.argmax(perc_agg_2)/len(perc_agg_2)\n",
    "        perc_list.append(max_perc)\n",
    "\n",
    "    np.savetxt(X=perc_list, fname=working_dir+\"/topology/perc/original/\"+nam+\".txt\", fmt=\"%.5f\")\n",
    "\n",
    "#for randomizations\n",
    "\n",
    "for k in range(0,1): # in this case done for the first randomized system, enlarge the number to get more\n",
    "    if not os.path.isdir(working_dir+\"/topology/perc/\"+str(k)):\n",
    "        os.mkdir(working_dir+\"/topology/perc/\"+str(k))\n",
    "    for nam, lst in zip(names_lists, index_lists):\n",
    "        print(nam)\n",
    "        perc_list=[]\n",
    "        for i in tqdm(range(len(lst))):\n",
    "            net = mxp.VirusMultiplex_from_dirlist([working_dir+\"/SynteticViruses/\"+str(k)+\"/\"+a for a in virus_metadata.iloc[lst[i]][\"virus\"]])\n",
    "            \n",
    "            tensor=mxp.build.get_node_tensor_from_network_list(net.g_list)\n",
    "\n",
    "            order = mxp.versatility.get_multi_RW_centrality_edge_colored(tensor)\n",
    "            order = order.sort_values(\"vers\")[\"phy nodes\"].to_numpy()\n",
    "            g_agg = mxp.build.get_aggregate_network(tensor)\n",
    "    \n",
    "            perc_agg_2 = gt.topology.vertex_percolation(g_agg, order, second=True)[0]\n",
    "            max_perc = np.argmax(perc_agg_2)/len(perc_agg_2)\n",
    "            perc_list.append(max_perc)\n",
    "    \n",
    "        np.savetxt(X=perc_list, fname=working_dir+\"/topology/perc/\"+str(k)+\"/\"+nam+\".txt\", fmt=\"%.5f\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Community partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get number of not-empty modules and the modularity of the multilayers samples from the combination sets\n",
    "# using the degree corrected stochastic block model algorithm\n",
    "\n",
    "#original network\n",
    "\n",
    "if not os.path.isdir(working_dir+\"/topology/mods/original\"):\n",
    "    os.mkdir(working_dir+\"/topology/mods/original\")\n",
    "if not os.path.isdir(working_dir+\"/topology/mody/original\"):\n",
    "    os.mkdir(working_dir+\"/topology/mody/original\")\n",
    "for nam, lst in zip(names_lists, index_lists):\n",
    "    print(nam)\n",
    "    mods_list=[]\n",
    "    mody_list=[]\n",
    "    for i in tqdm(range(len(lst))):\n",
    "        net = mxp.VirusMultiplex_from_dirlist([working_dir+\"/SynteticViruses/original/\"+a for a in virus_metadata.iloc[lst[i]][\"virus\"]])\n",
    "        \n",
    "        mod_res = mxp.mesoscale.get_mod(g_multi=net.g_multi, n_iter=1)\n",
    "        mods_list.append(mod_res[0])\n",
    "        mody_list.append(mod_res[1])\n",
    "\n",
    "    np.savetxt(X=mods_list, fname=working_dir+\"/topology/mods/original/\"+nam+\".txt\", fmt=\"%d\")\n",
    "    np.savetxt(X=mody_list, fname=working_dir+\"/topology/mody/original/\"+nam+\".txt\", fmt=\"%.5f\")\n",
    "\n",
    "#randomizations\n",
    "\n",
    "for k in range(0,1): # in this case done for the first randomized system, enlarge the number to get more\n",
    "    if not os.path.isdir(working_dir+\"/topology/mods/\"+str(k)):\n",
    "        os.mkdir(working_dir+\"/topology/mods/\"+str(k))\n",
    "    if not os.path.isdir(working_dir+\"/topology/mody/\"+str(k)):\n",
    "        os.mkdir(working_dir+\"/topology/mody/\"+str(k))\n",
    "    for nam, lst in zip(names_lists, index_lists):\n",
    "        print(nam)\n",
    "        mods_list=[]\n",
    "        mody_list=[]\n",
    "        for i in tqdm(range(len(lst))):\n",
    "            net = mxp.VirusMultiplex_from_dirlist([working_dir+\"/SynteticViruses/\"+str(k)+\"/\"+a for a in virus_metadata.iloc[lst[i]][\"virus\"]])\n",
    "            \n",
    "            mod_res = mxp.mesoscale.get_mod(g_multi=net.g_multi, n_iter=1)\n",
    "            mods_list.append(mod_res[0])\n",
    "            mody_list.append(mod_res[1])\n",
    "    \n",
    "        np.savetxt(X=mods_list, fname=working_dir+\"/topology/mods/\"+str(k)+\"/\"+nam+\".txt\", fmt=\"%d\")\n",
    "        np.savetxt(X=mody_list, fname=working_dir+\"/topology/mody/\"+str(k)+\"/\"+nam+\".txt\", fmt=\"%.5f\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏                                      | 11/2000 [00:37<1:33:09,  2.81s/it]"
     ]
    }
   ],
   "source": [
    "# producing vectors of floats containing the values of the multi-pagerank centrality measure calculated\n",
    "# for samples belonging to the n and n1o combination sets, to be used for the machine learning phase\n",
    "\n",
    "# the final vectors are of the size of the number of nodes in the entire human PPI net, and each position\n",
    "# correpsponds to a specific human protein, in order to create a common framework for all the samples\n",
    "\n",
    "#building a ID, protein names map to construct the final centrality vectors in a uniform way\n",
    "biostr_df = pd.read_csv(working_dir+\"/data_BIOGRID/BIOGRID_homo_sapiens.nodes\", sep=\" \")\n",
    "biostr_map = dict(zip(biostr_df[\"nodeSymbol\"], biostr_df[\"nodeID\"]))\n",
    "\n",
    "\n",
    "for i in tqdm(range(3000)):\n",
    "    for j in [0,1]:          ##\n",
    "        nam=names_lists[j]   ## cosidering only samples form n and n1o combination sets\n",
    "        lst=index_lists[j]   ##\n",
    "        if not os.path.isdir(working_dir+\"/topology/centrality/\"+nam):\n",
    "            os.mkdir(working_dir+\"/topology/centrality/\"+nam)\n",
    "        \n",
    "        net = mxp.VirusMultiplex(lst[i], target_folder=target_folder, virus_metadata=virus_metadata)\n",
    "        tensor = mxp.build.get_node_tensor_from_network_list(net.g_list)\n",
    "        \n",
    "        \n",
    "        res_df = mxp.versatility.get_multi_RW_centrality_edge_colored(node_tensor=tensor, cval=0.15)\n",
    "        \n",
    "\n",
    "        list_res = np.array(list(net.node_map.keys()))[res_df.sort_values(\"vers\", ascending=False).index[:50]]\n",
    "\n",
    "        centr_norm = np.zeros(len(biostr_map))\n",
    "        centr_norm[np.array(itemgetter(*list(net.node_map.keys()))(biostr_map))] = res_df[\"vers\"].to_numpy()\n",
    "        centr_norm=centr_norm/max(centr_norm)\n",
    "        \n",
    "        np.savetxt(X=centr_norm, fname=working_dir+\"/topology/centrality/\"+nam+\"/\"+str(i)+\".txt\", fmt=\"%.4e\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compl",
   "language": "python",
   "name": "compl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
