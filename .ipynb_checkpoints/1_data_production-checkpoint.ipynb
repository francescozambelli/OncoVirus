{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import graph_tool as gt\n",
    "from graph_tool.all import vertex_percolation\n",
    "from tqdm import tqdm\n",
    "from operator import itemgetter\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "import MuxVizPy as mxp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_nodes = pd.read_csv(\"data/BIOSTR_homo_sapiens.nodes\", sep=\" \")\n",
    "human_map = dict(zip(human_nodes['nodeSymbol'], np.arange(len(human_nodes))))\n",
    "\n",
    "virus_names = np.sort(os.listdir(\"data/SyntheticViruses/original\"))\n",
    "virus_metadata = pd.read_csv(\"data/viruses_metadata.csv\", header=0, sep=\";\")\n",
    "virus_onco = virus_metadata[virus_metadata[\"isOncogenic\"] == True].virus.unique()\n",
    "virus_nonco = virus_metadata[virus_metadata[\"isOncogenic\"] == False].virus.unique()\n",
    "\n",
    "synt_files = np.sort(os.listdir(working_dir+\"/SyntheticViruses/\"))[:5]\n",
    "\n",
    "virus_dict = {i : vname for i, vname in enumerate(virus_names)}\n",
    "\n",
    "virus_onco_idx = np.where(np.isin(virus_names, virus_onco))[0]\n",
    "virus_nonco_idx = np.where(np.isin(virus_names, virus_nonco))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index Lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick random samples of oncogenic and non-oncogenic viruses in different proportion to generate the combination sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_iters = 256\n",
    "np.random.seed(100)\n",
    "\n",
    "#N\n",
    "n_virus_indexes = np.array([np.random.choice(virus_nonco_idx, 4, replace=False) for i in range(n_iters)])\n",
    "\n",
    "#N1O\n",
    "n1o_virus_indexes = []\n",
    "for i in range(n_iters):\n",
    "    onco_pick = np.random.choice(virus_onco_idx, 1, replace=False)\n",
    "    nonco_pick = np.random.choice(virus_nonco_idx, 3, replace=False)\n",
    "    n1o_virus_indexes.append(np.concatenate([nonco_pick, onco_pick]))\n",
    "n10_virus_indexes = np.array(n1o_virus_indexes)\n",
    "\n",
    "#N2O\n",
    "n2o_virus_indexes = []\n",
    "for i in range(n_iters):\n",
    "    onco_pick = np.random.choice(virus_onco_idx, 2, replace=False)\n",
    "    nonco_pick = np.random.choice(virus_nonco_idx, 2, replace=False)\n",
    "    n2o_virus_indexes.append(np.concatenate([nonco_pick, onco_pick]))\n",
    "n20_virus_indexes = np.array(n2o_virus_indexes)\n",
    "\n",
    "#N3O\n",
    "n3o_virus_indexes = []\n",
    "for i in range(n_iters):\n",
    "    onco_pick = np.random.choice(virus_onco_idx, 3, replace=False)\n",
    "    nonco_pick = np.random.choice(virus_nonco_idx, 1, replace=False)\n",
    "    n3o_virus_indexes.append(np.concatenate([nonco_pick, onco_pick]))\n",
    "n3o_virus_indexes = np.array(n3o_virus_indexes)\n",
    "\n",
    "#O\n",
    "comb = list(itertools.combinations(range(8), 4))\n",
    "o_virus_indexes = np.array([list(virus_onco_idx[list(comb[i])]) for i in range(len(comb))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of n: 256\n",
      "Length of n1o: 256\n",
      "Length of n2o: 256\n",
      "Length of n3o: 256\n",
      "Length of o: 70\n"
     ]
    }
   ],
   "source": [
    "#organize in lists to better handle later\n",
    "\n",
    "index_lists = [n_virus_indexes, n1o_virus_indexes, n2o_virus_indexes, n3o_virus_indexes, o_virus_indexes]\n",
    "names_lists=[\"n\", \"n1o\", \"n2o\", \"n3o\", \"o\"]\n",
    "\n",
    "for i in range(len(index_lists)):\n",
    "    print(f\"Length of {names_lists[i]}: {len(index_lists[i])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the indexes combination for reproducibility    \n",
    "for i in range(len(index_lists)):\n",
    "    np.savetxt(X=index_lists[i], fname=\"data/MultilayerIndexes/\"+names_lists[i]+\".txt\", fmt=\"%d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topological quantities extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis of the Largest Intersected Components (LICs) allows us to investigate common features among viruses within the same multilayer network, particularly by identifying a shared core of nodes involved in the PPIs of viruses in the same category. Additionally, the LIC can be compared with the corresponding **Largest Viable Component (LVC)**, which is a subset of the LIC and represents the set of nodes that are simultaneously connected by the same path in all layers.\n",
    "\n",
    "<img src=\"images/LVC.png\" alt=\"lvc\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LVC computation for each set of combinations ###\n",
    "####################################################\n",
    "\n",
    "if not os.path.isdir(working_dir+\"/LVC/original\"):\n",
    "    os.mkdir(working_dir+\"/LVC/original\")\n",
    "for nam, lst in zip(names_lists, index_lists):\n",
    "    print(\"Computing LVC for set:\", nam)\n",
    "    lvc_size=[]\n",
    "    for i in tqdm(range(len(lst))):\n",
    "        net = mxp.VirusMultiplex_from_dirlist([working_dir+\"/SyntheticViruses/original/\"+a for a in itemgetter(*lst[i])(virus_dict)])\n",
    "        lvc_curr = mxp.topology.get_multi_LVC(net.g_list, printt=False)\n",
    "        if type(lvc_curr)==int:\n",
    "            lvc_curr = [lvc_curr]\n",
    "        lvc_size.append(len(lvc_curr))\n",
    "\n",
    "    np.savetxt(X=lvc_size, fname=working_dir+\"/LVC/original/\"+nam+\".txt\", fmt=\"%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing synthetic set: 0\n",
      "Computing LVC for set: n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256/256 [03:22<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing LVC for set: n1o\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 250/256 [06:13<00:08,  1.41s/it]"
     ]
    }
   ],
   "source": [
    "\n",
    "#for each realization of the randomization of the human PPI network\n",
    "for s in synt_files:\n",
    "    print(\"Processing synthetic set:\", s)\n",
    "    if not os.path.isdir(working_dir+\"/LVC/\"+str(s)):\n",
    "        os.mkdir(working_dir+\"/LVC/\"+str(s))\n",
    "    for nam, lst in zip(names_lists, index_lists):\n",
    "        print(\"Computing LVC for set:\", nam)\n",
    "        lvc_size=[]\n",
    "        for i in tqdm(range(len(lst))):\n",
    "            net = mxp.VirusMultiplex_from_dirlist([working_dir+\"/SyntheticViruses/\"+str(s)+\"/\"+a for a in itemgetter(*lst[i])(virus_dict)])\n",
    "            \n",
    "            lvc_curr = mxp.topology.get_multi_LVC(net.g_list, printt=False)\n",
    "    \n",
    "            if type(lvc_curr)==int:\n",
    "                lvc_curr = [lvc_curr]\n",
    "            lvc_size.append(len(lvc_curr))\n",
    "    \n",
    "        np.savetxt(X=lvc_size, fname=working_dir+\"/LVC/\"+str(s)+\"/\"+nam+\"_lvc.txt\", fmt=\"%d\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the list of nodes in the LVCs\n",
    "# the resulting file will be used for the GO enrichment analysis\n",
    "\n",
    "# original network\n",
    "net = mxp.VirusMultiplex_from_dirlist([working_dir+\"/SynteticViruses/original/\"+a for a in virus_metadata.iloc[onco_virus_indexes][\"virus\"]])  \n",
    "lvc_curr = mxp.topology.get_multi_LVC(net.g_list, printt=False)\n",
    "np.savetxt(X=np.array(list(net.node_map))[lvc_curr], fname=working_dir+\"/GOdata/genes.list\", fmt=\"%s\")\n",
    "\n",
    "# saving union of nodes in onco layers of the original net\n",
    "# to be used as background gene set for the GO enrichment analysis\n",
    "#np.savetxt(X=list(net.node_map.keys()), fname=working_dir+\"/GOdata/gobackground.list\", fmt=\"%s\")\n",
    "\n",
    "#randomized networks\n",
    "for k in tqdm(range(500)):\n",
    "    net = mxp.VirusMultiplex_from_dirlist([working_dir+\"/SynteticViruses/\"+str(k)+\"/\"+a for a in virus_metadata.iloc[onco_virus_indexes][\"virus\"]])\n",
    "            \n",
    "    lvc_curr = mxp.topology.get_multi_LVC(net.g_list, printt=False)\n",
    "    lvc_synt=np.array(list(net.node_map))[lvc_curr]\n",
    "    np.savetxt(X=lvc_synt, fname=working_dir+\"/GOdata/Synt/genes_\"+str(k)+\".list\", fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Percolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach to analyze the properties of the systems is to examine their **robustness to percolation processes**. Percolation, specifically nodes percolation, involves progressively removing nodes from the networks. The order in which nodes are removed, along with their corresponding links, in the multilayer networks is determined by their descending order of pagerank versatility when considering the system as an edge-colored network.\n",
    "In this analysis, the focus is on the critical point, which refers to the fraction of removed nodes at which a peak in the dimension of the second largest component is observed. This critical point provides insights into the network’s resistance to targeted attacks, which, from a biological perspective, could correspond to the action of certain drugs.\n",
    "\n",
    "<img src=\"images/criticPoint.png\" alt=\"percolation\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing percolation points for set: n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256/256 [02:47<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing percolation points for set: n1o\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256/256 [04:36<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing percolation points for set: n2o\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256/256 [06:54<00:00,  1.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing percolation points for set: n3o\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256/256 [07:50<00:00,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing percolation points for set: o\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [02:34<00:00,  2.21s/it]\n"
     ]
    }
   ],
   "source": [
    "### Node percolation (page-rank) analysis ###\n",
    "#############################################\n",
    "\n",
    "if not os.path.isdir(working_dir+\"/percolation/original\"):\n",
    "    os.mkdir(working_dir+\"/percolation/original\")\n",
    "for nam, lst in zip(names_lists, index_lists):\n",
    "    print(\"Computing percolation points for set:\", nam)\n",
    "    perc_list=[]\n",
    "    for i in tqdm(range(len(lst))):\n",
    "        # create multiplex network for the current sample\n",
    "        net = mxp.VirusMultiplex_from_dirlist([working_dir+\"/SyntheticViruses/original/\"+a for a in itemgetter(*lst[i])(virus_dict)])\n",
    "        tensor=mxp.build.get_node_tensor_from_network_list(net.g_list)\n",
    "\n",
    "        # define node removal order based on pagerank versatility\n",
    "        order = mxp.versatility.get_multi_RW_centrality_edge_colored(tensor)\n",
    "        order = order.sort_values(\"vers\")[\"phy nodes\"].to_numpy()\n",
    "        g_agg = mxp.build.get_aggregate_network(tensor)\n",
    "\n",
    "        # compute critical point based on second largest component size\n",
    "        perc_agg_2 = vertex_percolation(g_agg, order, second=True)[0]\n",
    "        max_perc = np.argmax(perc_agg_2)/len(perc_agg_2)\n",
    "        perc_list.append(max_perc)\n",
    "\n",
    "    np.savetxt(X=perc_list, fname=working_dir+\"/percolation/original/\"+nam+\".txt\", fmt=\"%.5f\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 256/256 [02:33<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n1o\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 256/256 [04:14<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n2o\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 256/256 [05:36<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n3o\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 256/256 [04:00<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 70/70 [02:33<00:00,  2.19s/it]\n"
     ]
    }
   ],
   "source": [
    "#for randomizations\n",
    "\n",
    "for k in range(0,1): # in this case done for the first randomized system, enlarge the number to get more\n",
    "    if not os.path.isdir(working_dir+\"/topology/percolation/\"+str(k)):\n",
    "        os.mkdir(working_dir+\"/topology/percolation/\"+str(k))\n",
    "    for nam, lst in zip(names_lists, index_lists):\n",
    "        print(nam)\n",
    "        perc_list=[]\n",
    "        for i in tqdm(range(len(lst))):\n",
    "            net = mxp.VirusMultiplex_from_dirlist([working_dir+\"/SynteticViruses/\"+str(k)+\"/\"+a for a in virus_metadata.iloc[lst[i]][\"virus\"]])\n",
    "            \n",
    "            tensor=mxp.build.get_node_tensor_from_network_list(net.g_list)\n",
    "\n",
    "            order = mxp.versatility.get_multi_RW_centrality_edge_colored(tensor)\n",
    "            order = order.sort_values(\"vers\")[\"phy nodes\"].to_numpy()\n",
    "            g_agg = mxp.build.get_aggregate_network(tensor)\n",
    "    \n",
    "            perc_agg_2 = gt.topology.vertex_percolation(g_agg, order, second=True)[0]\n",
    "            max_perc = np.argmax(perc_agg_2)/len(perc_agg_2)\n",
    "            perc_list.append(max_perc)\n",
    "    \n",
    "        np.savetxt(X=perc_list, fname=working_dir+\"/topology/percolation/\"+str(k)+\"/\"+nam+\".txt\", fmt=\"%.5f\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mesoscale structure of the multilayer networks is further explored by comparing the results of the **Degree-corrected Stochastic Block Model (DCSBM)**. \n",
    "\n",
    "The 2 analyzed quantities are: \n",
    "- the number of non-empty modules \n",
    "- the modularity of the systems.\n",
    "\n",
    "<img src=\"images/SBM.png\" alt=\"sbm\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing block structure for set: n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/256 [00:00<?, ?it/s]\n",
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:10<00:00, 10.41s/it]\u001b[A\n",
      "  0%|▏                                          | 1/256 [00:10<45:10, 10.63s/it]\n",
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "### DCSBM block structure analysis ###\n",
    "######################################\n",
    "\n",
    "if not os.path.isdir(working_dir+\"/modules/original\"):\n",
    "    os.mkdir(working_dir+\"/modules/original\")\n",
    "if not os.path.isdir(working_dir+\"/modularity/original\"):\n",
    "    os.mkdir(working_dir+\"/modularity/original\")\n",
    "for nam, lst in zip(names_lists, index_lists):\n",
    "    print(\"Analyzing block structure for set:\", nam)\n",
    "    mods_list=[]\n",
    "    mody_list=[]\n",
    "    for i in tqdm(range(len(lst))):\n",
    "        net = mxp.VirusMultiplex_from_dirlist([working_dir+\"/SyntheticViruses/original/\"+a for a in virus_metadata.iloc[lst[i]][\"virus\"]])\n",
    "        \n",
    "        mod_res = mxp.mesoscale.get_mod(g_multi=net.g_multi, n_iter=1)\n",
    "        mods_list.append(mod_res[0])\n",
    "        mody_list.append(mod_res[1])\n",
    "\n",
    "    np.savetxt(X=mods_list, fname=working_dir+\"/modules/original/\"+nam+\".txt\", fmt=\"%d\")\n",
    "    np.savetxt(X=mody_list, fname=working_dir+\"/modularity/original/\"+nam+\".txt\", fmt=\"%.5f\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomizations\n",
    "\n",
    "for k in range(0,1): # in this case done for the first randomized system, enlarge the number to get more\n",
    "    if not os.path.isdir(working_dir+\"/modules/\"+str(k)):\n",
    "        os.mkdir(working_dir+\"/modules/\"+str(k))\n",
    "    if not os.path.isdir(working_dir+\"/modularity/\"+str(k)):\n",
    "        os.mkdir(working_dir+\"/modularity/\"+str(k))\n",
    "    for nam, lst in zip(names_lists, index_lists):\n",
    "        print(nam)\n",
    "        mods_list=[]\n",
    "        mody_list=[]\n",
    "        for i in tqdm(range(len(lst))):\n",
    "            net = mxp.VirusMultiplex_from_dirlist([working_dir+\"/SyntheticViruses/\"+str(k)+\"/\"+a for a in virus_metadata.iloc[lst[i]][\"virus\"]])\n",
    "            \n",
    "            mod_res = mxp.mesoscale.get_mod(g_multi=net.g_multi, n_iter=1)\n",
    "            mods_list.append(mod_res[0])\n",
    "            mody_list.append(mod_res[1])\n",
    "    \n",
    "        np.savetxt(X=mods_list, fname=working_dir+\"/modules/\"+str(k)+\"/\"+nam+\".txt\", fmt=\"%d\")\n",
    "        np.savetxt(X=mody_list, fname=working_dir+\"/modularity/\"+str(k)+\"/\"+nam+\".txt\", fmt=\"%.5f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data for ML classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iters_ML = 3000\n",
    "np.random.seed(100)\n",
    "\n",
    "#N\n",
    "n_virus_indexes_ML = np.array([np.random.choice(virus_nonco_idx, 4, replace=False) for i in range(n_iters_ML)])\n",
    "\n",
    "#N1O\n",
    "n1o_virus_indexes_ML = []\n",
    "for i in range(n_iters_ML):\n",
    "    onco_pick = np.random.choice(virus_onco_idx, 1, replace=False)\n",
    "    nonco_pick = np.random.choice(virus_nonco_idx, 3, replace=False)\n",
    "    n1o_virus_indexes_ML.append(np.concatenate([nonco_pick, onco_pick]))\n",
    "n1o_virus_indexes_ML = np.array(n1o_virus_indexes_ML)\n",
    "\n",
    "#np.savetxt(X=n_virus_indexes_ML, fname=\"data/MultilayerIndexes/n_ML.txt\", fmt=\"%d\")\n",
    "#np.savetxt(X=n1o_virus_indexes_ML, fname=\"data/MultilayerIndexes/n1o_ML.txt\", fmt=\"%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################CENTR ONLY###########################################################\n",
    "n_top = 2000\n",
    "np.random.seed(100)\n",
    "\n",
    "for nam, lst in zip([\"n\", \"n1o\"], [n_virus_indexes_ML, n1o_virus_indexes_ML]):\n",
    "    centr_vec = []\n",
    "    pos_vec = []\n",
    "    for i in tqdm(range(n_iters_ML)):\n",
    "        net = mxp.VirusMultiplex_from_dirlist([working_dir+\"/SyntheticViruses/original/\"+a for a in itemgetter(*lst[i])(virus_dict)])\n",
    "        tensor = mxp.build.get_node_tensor_from_network_list(net.g_list)\n",
    "        \n",
    "        res_df = mxp.versatility.get_multi_RW_centrality_edge_colored(node_tensor=tensor, cval=0.15)\n",
    "\n",
    "        list_res = np.array(list(net.node_map.keys()))[res_df.sort_values(\"vers\", ascending=False).index[:50]]\n",
    "\n",
    "        centr_norm = np.zeros(len(human_nodes))\n",
    "        centr_norm[np.array(itemgetter(*list(net.node_map.keys()))(human_map))] = res_df[\"vers\"].to_numpy()\n",
    "        centr_norm=centr_norm/max(centr_norm)\n",
    "\n",
    "        # get top 2000 nodes and save their normalized centrality values with position in the human PPI network\n",
    "        top_nodes = np.argsort(centr_norm)[-n_top:]\n",
    "        centr_vec.append(centr_norm[top_nodes])\n",
    "        pos_vec.append(top_nodes)\n",
    "\n",
    "    np.savetxt(X=centr_vec, fname=working_dir+\"/ML_data/\"+nam+\".txt\", fmt=\"%.4e\")\n",
    "    np.savetxt(X=pos_vec, fname=working_dir+\"/ML_data/\"+nam+\"_pos.txt\", fmt=\"%d\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
